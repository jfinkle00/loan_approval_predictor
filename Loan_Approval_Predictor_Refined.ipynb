{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Loan Approval Prediction Model\n",
    "\n",
    "**Author:** Jason Finkle  \n",
    "**Project:** Financial Services ML Classification  \n",
    "\n",
    "This notebook develops and compares multiple machine learning models to predict loan approval decisions based on applicant financial characteristics. The project demonstrates end-to-end ML workflow including data preprocessing, model training with cross-validation, and comprehensive evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, roc_curve, auc,\n",
    "                             precision_recall_curve, average_precision_score)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create figures directory\n",
    "import os\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "# Define color palette\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',\n",
    "    'secondary': '#A23B72',\n",
    "    'accent': '#F18F01',\n",
    "    'success': '#28A745',\n",
    "    'danger': '#DC3545',\n",
    "    'dark': '#343A40'\n",
    "}\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Observations: {len(df):,}\")\n",
    "print(f\"Features: {len(df.columns)}\")\n",
    "print(f\"\\nColumn Names: {list(df.columns)}\")\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and info\n",
    "print(\"üìã DATA TYPES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"\\nüìà STATISTICAL SUMMARY\")\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and duplicates\n",
    "print(\"üîç DATA QUALITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nDuplicate Rows: {df.duplicated().sum():,}\")\n",
    "print(f\"\\n‚úÖ No missing values detected\" if df.isnull().sum().sum() == 0 else \"‚ö†Ô∏è Missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"üéØ TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\" * 50)\n",
    "approval_counts = df['Approval'].value_counts()\n",
    "print(approval_counts)\n",
    "print(f\"\\nApproval Rate: {(approval_counts['Approved'] / len(df) * 100):.1f}%\")\n",
    "print(f\"Rejection Rate: {(approval_counts['Rejected'] / len(df) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Target Distribution\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "colors_target = [COLORS['success'], COLORS['danger']]\n",
    "bars = ax.bar(approval_counts.index, approval_counts.values, color=colors_target, \n",
    "              width=0.6, edgecolor='white', linewidth=2)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, approval_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200, \n",
    "            f'{val:,}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_title('Loan Approval Distribution', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Decision', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_ylim(0, max(approval_counts.values) * 1.15)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/01_target_distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/01_target_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Feature Distributions\n",
    "numeric_cols = ['Income', 'Credit_Score', 'Loan_Amount', 'DTI_Ratio']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot by approval status\n",
    "    for status, color in [('Approved', COLORS['success']), ('Rejected', COLORS['danger'])]:\n",
    "        data = df[df['Approval'] == status][col]\n",
    "        ax.hist(data, bins=30, alpha=0.6, label=status, color=color, edgecolor='white')\n",
    "    \n",
    "    ax.set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Feature Distributions by Approval Status', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/02_feature_distributions.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/02_feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Box Plots by Approval Status\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create box plot\n",
    "    bp = ax.boxplot([df[df['Approval'] == 'Approved'][col], \n",
    "                     df[df['Approval'] == 'Rejected'][col]],\n",
    "                    labels=['Approved', 'Rejected'],\n",
    "                    patch_artist=True)\n",
    "    \n",
    "    # Color the boxes\n",
    "    bp['boxes'][0].set_facecolor(COLORS['success'])\n",
    "    bp['boxes'][1].set_facecolor(COLORS['danger'])\n",
    "    for box in bp['boxes']:\n",
    "        box.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_title(f'{col} by Approval Status', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(col)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Feature Comparison: Approved vs Rejected', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/03_boxplots_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/03_boxplots_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 4: Correlation Heatmap\n",
    "# Prepare data for correlation\n",
    "df_corr = df.copy()\n",
    "df_corr['Approval_Binary'] = (df_corr['Approval'] == 'Approved').astype(int)\n",
    "df_corr['Employment_Binary'] = (df_corr['Employment_Status'] == 'employed').astype(int)\n",
    "\n",
    "corr_cols = ['Income', 'Credit_Score', 'Loan_Amount', 'DTI_Ratio', 'Employment_Binary', 'Approval_Binary']\n",
    "corr_matrix = df_corr[corr_cols].corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, square=True, linewidths=1, cbar_kws={'shrink': 0.8},\n",
    "            ax=ax, vmin=-1, vmax=1)\n",
    "\n",
    "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/04_correlation_heatmap.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/04_correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5: Employment Status vs Approval\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "employment_approval = pd.crosstab(df['Employment_Status'], df['Approval'], normalize='index') * 100\n",
    "\n",
    "x = np.arange(len(employment_approval.index))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, employment_approval['Approved'], width, \n",
    "               label='Approved', color=COLORS['success'], edgecolor='white')\n",
    "bars2 = ax.bar(x + width/2, employment_approval['Rejected'], width, \n",
    "               label='Rejected', color=COLORS['danger'], edgecolor='white')\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 1,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "ax.set_title('Approval Rate by Employment Status', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Employment Status', fontsize=12)\n",
    "ax.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([s.title() for s in employment_approval.index])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 100)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/05_employment_approval.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/05_employment_approval.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for modeling\n",
    "df_model = df.copy()\n",
    "\n",
    "# Drop text column (not used for this analysis)\n",
    "df_model = df_model.drop('Text', axis=1)\n",
    "\n",
    "# One-hot encode Employment_Status\n",
    "df_model = pd.get_dummies(df_model, columns=['Employment_Status'], drop_first=True)\n",
    "\n",
    "# Encode target variable\n",
    "le = LabelEncoder()\n",
    "df_model['Approval'] = le.fit_transform(df_model['Approval'])  # Approved=0, Rejected=1\n",
    "# Flip so Approved=1 (positive class)\n",
    "df_model['Approval'] = 1 - df_model['Approval']\n",
    "\n",
    "print(\"üìã PREPROCESSED DATA\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df_model.shape}\")\n",
    "print(f\"\\nFeatures: {list(df_model.columns)}\")\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(df_model['Approval'].value_counts())\n",
    "print(\"\\n(1 = Approved, 0 = Rejected)\")\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df_model.drop('Approval', axis=1)\n",
    "y = df_model['Approval']\n",
    "\n",
    "# Scale numeric features\n",
    "scaler = MinMaxScaler()\n",
    "numeric_features = ['Credit_Score', 'Income', 'Loan_Amount', 'DTI_Ratio']\n",
    "X[numeric_features] = scaler.fit_transform(X[numeric_features])\n",
    "\n",
    "# Train-test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(\"üìä TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"\\nTraining approval rate: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"Test approval rate: {y_test.mean()*100:.1f}%\")\n",
    "print(f\"\\nFeatures: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE),\n",
    "    'SVM': SVC(kernel='linear', probability=True, random_state=RANDOM_STATE),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "cv_scores = {}\n",
    "\n",
    "print(\"ü§ñ TRAINING MODELS WITH 5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüìà Training {name}...\")\n",
    "    \n",
    "    # Cross-validation\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    cv_scores[name] = scores\n",
    "    print(f\"   CV Accuracy: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "    \n",
    "    # Fit on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'y_pred': y_pred,\n",
    "        'y_prob': y_prob,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'cv_mean': scores.mean(),\n",
    "        'cv_std': scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"   Test Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Voting Classifier (Ensemble)\n",
    "print(\"\\nüó≥Ô∏è Training Voting Classifier (Ensemble)...\")\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', models['Logistic Regression']),\n",
    "        ('rf', models['Random Forest']),\n",
    "        ('svm', models['SVM']),\n",
    "        ('gb', models['Gradient Boosting'])\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(voting_clf, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "cv_scores['Voting Ensemble'] = scores\n",
    "print(f\"   CV Accuracy: {scores.mean():.4f} (+/- {scores.std()*2:.4f})\")\n",
    "\n",
    "# Fit and predict\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "y_prob_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "results['Voting Ensemble'] = {\n",
    "    'model': voting_clf,\n",
    "    'y_pred': y_pred_voting,\n",
    "    'y_prob': y_prob_voting,\n",
    "    'accuracy': accuracy_score(y_test, y_pred_voting),\n",
    "    'precision': precision_score(y_test, y_pred_voting),\n",
    "    'recall': recall_score(y_test, y_pred_voting),\n",
    "    'f1': f1_score(y_test, y_pred_voting),\n",
    "    'cv_mean': scores.mean(),\n",
    "    'cv_std': scores.std()\n",
    "}\n",
    "\n",
    "print(f\"   Test Accuracy: {results['Voting Ensemble']['accuracy']:.4f}\")\n",
    "print(\"\\n‚úÖ Voting Ensemble trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics summary DataFrame\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'CV Accuracy': [f\"{results[m]['cv_mean']:.4f} ¬± {results[m]['cv_std']:.4f}\" for m in results],\n",
    "    'Test Accuracy': [results[m]['accuracy'] for m in results],\n",
    "    'Precision': [results[m]['precision'] for m in results],\n",
    "    'Recall': [results[m]['recall'] for m in results],\n",
    "    'F1 Score': [results[m]['f1'] for m in results]\n",
    "}).round(4)\n",
    "\n",
    "# Sort by F1 Score\n",
    "metrics_df = metrics_df.sort_values('F1 Score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìä MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv('model_metrics_summary.csv', index=False)\n",
    "print(\"\\n‚úÖ Metrics saved to: model_metrics_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 6: ROC Curves Comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "colors_list = [COLORS['primary'], COLORS['secondary'], COLORS['accent'], \n",
    "               COLORS['success'], COLORS['danger']]\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    fpr, tpr, _ = roc_curve(y_test, res['y_prob'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    ax.plot(fpr, tpr, color=colors_list[idx], lw=2,\n",
    "            label=f\"{name} (AUC = {roc_auc:.3f})\")\n",
    "\n",
    "ax.plot([0, 1], [0, 1], 'k--', lw=1.5, alpha=0.7, label='Random Classifier')\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves: Model Comparison', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/06_roc_curves.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/06_roc_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 7: Metrics Comparison Bar Chart\n",
    "metrics_to_plot = ['Test Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "model_names = list(results.keys())\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Get values for this metric\n",
    "    if metric == 'Test Accuracy':\n",
    "        values = [results[m]['accuracy'] for m in model_names]\n",
    "    elif metric == 'Precision':\n",
    "        values = [results[m]['precision'] for m in model_names]\n",
    "    elif metric == 'Recall':\n",
    "        values = [results[m]['recall'] for m in model_names]\n",
    "    else:\n",
    "        values = [results[m]['f1'] for m in model_names]\n",
    "    \n",
    "    # Create horizontal bar chart\n",
    "    y_pos = np.arange(len(model_names))\n",
    "    bars = ax.barh(y_pos, values, color=colors_list, edgecolor='white', height=0.6)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(val + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{val:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(model_names)\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0, 1.1)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/07_metrics_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/07_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 8: Feature Importance (Random Forest)\n",
    "rf_model = results['Random Forest']['model']\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors_imp = plt.cm.Blues(np.linspace(0.4, 0.9, len(feature_importance)))\n",
    "bars = ax.barh(feature_importance['Feature'], feature_importance['Importance'],\n",
    "               color=colors_imp, edgecolor='white', height=0.6)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, feature_importance['Importance']):\n",
    "    ax.text(val + 0.005, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_title('Random Forest Feature Importance', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/08_feature_importance.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/08_feature_importance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 9: Logistic Regression Coefficients\n",
    "lr_model = results['Logistic Regression']['model']\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_model.coef_[0]\n",
    "}).sort_values('Coefficient', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors_coef = [COLORS['danger'] if c < 0 else COLORS['success'] for c in coefficients['Coefficient']]\n",
    "bars = ax.barh(coefficients['Feature'], coefficients['Coefficient'],\n",
    "               color=colors_coef, edgecolor='white', height=0.6)\n",
    "\n",
    "ax.axvline(x=0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, coefficients['Coefficient']):\n",
    "    offset = 0.05 if val >= 0 else -0.15\n",
    "    ax.text(val + offset, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.2f}', va='center', fontsize=10)\n",
    "\n",
    "ax.set_xlabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('Logistic Regression Coefficients\\n(Positive = Increases Approval Likelihood)', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/09_logistic_coefficients.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/09_logistic_coefficients.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 10: Confusion Matrix for Best Model\n",
    "best_model_name = metrics_df.iloc[0]['Model']\n",
    "best_results = results[best_model_name]\n",
    "\n",
    "cm = confusion_matrix(y_test, best_results['y_pred'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Rejected', 'Approved'],\n",
    "            yticklabels=['Rejected', 'Approved'],\n",
    "            annot_kws={'size': 16},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix: {best_model_name}\\n(Accuracy: {best_results[\"accuracy\"]:.1%})', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/10_confusion_matrix.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(f\"‚úÖ Saved: figures/10_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 11: Cross-Validation Score Distribution\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cv_data = [cv_scores[m] for m in cv_scores.keys()]\n",
    "positions = np.arange(len(cv_scores))\n",
    "\n",
    "bp = ax.boxplot(cv_data, positions=positions, patch_artist=True, widths=0.6)\n",
    "\n",
    "# Color the boxes\n",
    "for patch, color in zip(bp['boxes'], colors_list):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(list(cv_scores.keys()), rotation=15, ha='right')\n",
    "ax.set_ylabel('Accuracy Score', fontsize=12)\n",
    "ax.set_title('Cross-Validation Score Distribution (5-Fold)', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Add mean line\n",
    "means = [np.mean(cv_scores[m]) for m in cv_scores.keys()]\n",
    "ax.scatter(positions, means, color='red', s=100, zorder=5, marker='D', label='Mean')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/11_cv_distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/11_cv_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 12: Probability Distribution by Actual Class\n",
    "best_probs = best_results['y_prob']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Split probabilities by actual class\n",
    "prob_approved = best_probs[y_test == 1]\n",
    "prob_rejected = best_probs[y_test == 0]\n",
    "\n",
    "ax.hist(prob_rejected, bins=30, alpha=0.6, label='Actually Rejected', \n",
    "        color=COLORS['danger'], edgecolor='white')\n",
    "ax.hist(prob_approved, bins=30, alpha=0.6, label='Actually Approved', \n",
    "        color=COLORS['success'], edgecolor='white')\n",
    "\n",
    "ax.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "\n",
    "ax.set_xlabel('Predicted Probability of Approval', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title(f'Prediction Probability Distribution: {best_model_name}', \n",
    "             fontsize=14, fontweight='bold', pad=15)\n",
    "ax.legend()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/12_probability_distribution.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: figures/12_probability_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"                    üìä FINAL MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Best model\n",
    "best_model_name = metrics_df.iloc[0]['Model']\n",
    "best_f1 = metrics_df.iloc[0]['F1 Score']\n",
    "best_acc = metrics_df.iloc[0]['Test Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ Best Performing Model: {best_model_name}\")\n",
    "print(f\"   Test Accuracy: {best_acc:.4f}\")\n",
    "print(f\"   F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# Calculate AUC for best model\n",
    "fpr, tpr, _ = roc_curve(y_test, results[best_model_name]['y_prob'])\n",
    "best_auc = auc(fpr, tpr)\n",
    "print(f\"   AUC: {best_auc:.4f}\")\n",
    "\n",
    "print(\"\\nüìà All Models Ranked by F1 Score:\")\n",
    "print(metrics_df[['Model', 'Test Accuracy', 'F1 Score']].to_string(index=False))\n",
    "\n",
    "print(\"\\nüìÅ Figures generated:\")\n",
    "print(\"   01. Target distribution\")\n",
    "print(\"   02. Feature distributions\")\n",
    "print(\"   03. Box plots comparison\")\n",
    "print(\"   04. Correlation heatmap\")\n",
    "print(\"   05. Employment vs approval\")\n",
    "print(\"   06. ROC curves\")\n",
    "print(\"   07. Metrics comparison\")\n",
    "print(\"   08. Feature importance (RF)\")\n",
    "print(\"   09. Logistic coefficients\")\n",
    "print(\"   10. Confusion matrix\")\n",
    "print(\"   11. CV score distribution\")\n",
    "print(\"   12. Probability distribution\")\n",
    "\n",
    "print(\"\\nüîë Key Findings:\")\n",
    "print(\"   ‚Ä¢ Credit Score is the strongest predictor of loan approval\")\n",
    "print(\"   ‚Ä¢ DTI Ratio has significant negative impact on approval\")\n",
    "print(\"   ‚Ä¢ Employment status plays a crucial role in decisions\")\n",
    "print(\"   ‚Ä¢ Ensemble methods provide robust predictions\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"      Project by Jason Finkle | github.com/jfinkle00\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
